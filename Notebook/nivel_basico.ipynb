{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "![image](https://scontent.fudi1-1.fna.fbcdn.net/v/t1.6435-9/159228906_3690079687713312_5035028843487831689_n.png?_nc_cat=103&ccb=1-3&_nc_sid=e3f864&_nc_eui2=AeGxu7tQmrCVud2XV6HZGYLSRkoGovk4dmVGSgai-Th2ZXD5wS-ueGqt9aNhGL9Et1o&_nc_ohc=iD7uFTi5lIYAX8Iohin&_nc_ht=scontent.fudi1-1.fna&oh=f23b72635179e9c304be0b3999930447&oe=60EC6AC5)\n",
    "\n",
    "# Campanha Nacional de Vacinação contra Covid-19\n",
    "\n",
    "## **Etapas**\n",
    "1. Enviar os dados para o hdfs\n",
    "2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município.\n",
    "3. Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS\n",
    "4. Salvar a primeira visualização como tabela Hive\n",
    "5. Salvar a segunda visualização com formato parquet e compressão snappy\n",
    "6. Salvar a terceira visualização em um tópico no Kafka\n",
    "7. Criar a visualização pelo Spark com os dados enviados para o HDFS:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}